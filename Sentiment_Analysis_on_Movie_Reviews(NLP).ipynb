{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMkooB7ntCwxJ/lIv2luNwm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HammadSiddiqui30/Autonomous_Vehicle_Perception/blob/master/Sentiment_Analysis_on_Movie_Reviews(NLP).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Sentiment Analysis on Movie Reviews***"
      ],
      "metadata": {
        "id": "Rr8BOw_OIdbN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install nltk scikit-learn\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X1ymFcAKImO8",
        "outputId": "9e3d0bc7-7bfb-49c6-cf1f-989bbacceae5"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.3.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.5.15)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.4)\n",
            "Requirement already satisfied: numpy<2.0,>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import movie_reviews\n",
        "import random\n",
        "\n",
        "# Download NLTK data\n",
        "nltk.download('movie_reviews')\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Load movie reviews data\n",
        "documents = [(list(movie_reviews.words(fileid)), category)\n",
        "             for category in movie_reviews.categories()\n",
        "             for fileid in movie_reviews.fileids(category)]\n",
        "\n",
        "# Shuffle the documents to ensure randomness\n",
        "random.shuffle(documents)\n",
        "\n",
        "print(f'Total documents: {len(documents)}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v35v4sV2Imt3",
        "outputId": "8dd5f71f-f483-4fd1-ab7a-d3f72ba46056"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total documents: 2000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import string\n",
        "\n",
        "# Download stopwords\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def preprocess_text(text):\n",
        "    tokens = word_tokenize(' '.join(text))\n",
        "    tokens = [word.lower() for word in tokens if word.isalpha()]\n",
        "    tokens = [word for word in tokens if word not in stop_words]\n",
        "    return tokens\n",
        "\n",
        "# Preprocess the documents\n",
        "documents = [(preprocess_text(document), category) for document, category in documents]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TZRKRuObImwp",
        "outputId": "edd338b2-97fd-4fff-8ad5-d8401e52f2bf"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import FreqDist\n",
        "\n",
        "# Get the list of all words in the corpus\n",
        "all_words = [word for document, category in documents for word in document]\n",
        "all_words_freq = FreqDist(all_words)\n",
        "\n",
        "# Select the top 2000 most frequent words as features\n",
        "word_features = list(all_words_freq.keys())[:2000]\n",
        "\n",
        "def document_features(document):\n",
        "    document_words = set(document)\n",
        "    features = {}\n",
        "    for word in word_features:\n",
        "        features[f'contains({word})'] = (word in document_words)\n",
        "    return features\n",
        "\n",
        "# Create feature sets\n",
        "feature_sets = [(document_features(document), category) for document, category in documents]\n"
      ],
      "metadata": {
        "id": "grSen4ZWImzm"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from nltk.classify import NaiveBayesClassifier\n",
        "from nltk.classify.util import accuracy\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "train_set, test_set = train_test_split(feature_sets, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train the Naive Bayes classifier\n",
        "classifier = NaiveBayesClassifier.train(train_set)\n",
        "\n",
        "# Evaluate the classifier\n",
        "print(f'Accuracy: {accuracy(classifier, test_set) * 100:.2f}%')\n",
        "\n",
        "# Show the most informative features\n",
        "classifier.show_most_informative_features(10)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "83LWiMovIm2U",
        "outputId": "da1ec610-d2b3-4ad1-a2fc-50c19b4bf36c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 78.75%\n",
            "Most Informative Features\n",
            "    contains(astounding) = True              pos : neg    =     11.1 : 1.0\n",
            "        contains(debate) = True              pos : neg    =     11.1 : 1.0\n",
            "      contains(poignant) = True              pos : neg    =     10.3 : 1.0\n",
            "     contains(fashioned) = True              pos : neg    =      9.5 : 1.0\n",
            "   contains(outstanding) = True              pos : neg    =      9.4 : 1.0\n",
            "        contains(finest) = True              pos : neg    =      8.2 : 1.0\n",
            " contains(unimaginative) = True              neg : pos    =      7.6 : 1.0\n",
            "   contains(wonderfully) = True              pos : neg    =      6.9 : 1.0\n",
            "          contains(earl) = True              pos : neg    =      6.4 : 1.0\n",
            "       contains(layered) = True              pos : neg    =      6.4 : 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def classify_review(review_text):\n",
        "    review_tokens = preprocess_text(review_text)\n",
        "    review_features = document_features(review_tokens)\n",
        "    return classifier.classify(review_features)\n",
        "\n",
        "# Example review\n",
        "review = \"This movie was absolutely fantastic! The story was engaging and the characters were well-developed.\"\n",
        "print(f'Review: {review}')\n",
        "print(f'Sentiment: {classify_review(review)}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tio1RCN2Im5B",
        "outputId": "597cba20-b550-4468-c1ba-b208b475a07f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Review: This movie was absolutely fantastic! The story was engaging and the characters were well-developed.\n",
            "Sentiment: neg\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aaHeeL1kIm8H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aKXbw509Im_L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "r10389JbInBw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***NLTK WITH SVM***"
      ],
      "metadata": {
        "id": "kfhmhgt5L4xy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import movie_reviews\n",
        "import random\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Download NLTK data\n",
        "nltk.download('movie_reviews')\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Load movie reviews data\n",
        "documents = [(list(movie_reviews.words(fileid)), category)\n",
        "             for category in movie_reviews.categories()\n",
        "             for fileid in movie_reviews.fileids(category)]\n",
        "\n",
        "# Shuffle the documents to ensure randomness\n",
        "random.shuffle(documents)\n",
        "\n",
        "# Preview of the dataset\n",
        "print(f'Total documents: {len(documents)}')\n",
        "print(f'Sample document (first 100 words): {documents[0][0][:100]}')\n",
        "print(f'Category: {documents[0][1]}')\n",
        "\n",
        "# Define stop words\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # Tokenize the text (split it into individual words)\n",
        "    tokens = word_tokenize(' '.join(text))\n",
        "    # Convert words to lowercase and remove non-alphabetic tokens\n",
        "    tokens = [word.lower() for word in tokens if word.isalpha()]\n",
        "    # Remove stop words\n",
        "    tokens = [word for word in tokens if word not in stop_words]\n",
        "    return tokens\n",
        "\n",
        "# Preprocess the documents\n",
        "documents = [(preprocess_text(document), category) for document, category in documents]\n",
        "\n",
        "# Convert the documents into a format suitable for machine learning\n",
        "texts = [' '.join(document) for document, category in documents]\n",
        "labels = [category for document, category in documents]\n",
        "\n",
        "# Split the data into training and testing sets (80% train, 20% test)\n",
        "texts_train, texts_test, labels_train, labels_test = train_test_split(texts, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a pipeline with TfidfVectorizer and SVC\n",
        "pipeline = Pipeline([\n",
        "    ('tfidf', TfidfVectorizer()),\n",
        "    ('svc', SVC(kernel='linear'))\n",
        "])\n",
        "\n",
        "# Train the model\n",
        "pipeline.fit(texts_train, labels_train)\n",
        "\n",
        "# Evaluate the model\n",
        "predictions = pipeline.predict(texts_test)\n",
        "print(f'Accuracy: {accuracy_score(labels_test, predictions) * 100:.2f}%')\n",
        "\n",
        "# Function to classify a new review\n",
        "def classify_review(review_text):\n",
        "    return pipeline.predict([review_text])[0]\n",
        "\n",
        "# Example reviews to classify\n",
        "reviews = [\n",
        "    \"This movie was absolutely fantastic! The story was engaging and the characters were well-developed.\",\n",
        "    \"The movie was terrible. The plot was boring and the acting was awful.\",\n",
        "    \"An average movie with some good moments but overall not very memorable.\"\n",
        "]\n",
        "\n",
        "for review in reviews:\n",
        "    sentiment = classify_review(review)\n",
        "    print(f'Review: {review}')\n",
        "    print(f'Sentiment: {sentiment}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NDXL2ByvInEb",
        "outputId": "5b6cb364-8f29-4673-c59d-947aafb61c9a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n",
            "[nltk_data]   Package movie_reviews is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total documents: 2000\n",
            "Sample document (first 100 words): ['it', \"'\", 's', 'tough', 'to', 'really', 'say', 'something', 'nice', 'about', 'a', 'type', 'of', 'person', 'who', \"'\", 's', 'so', 'ethnocentric', 'that', 'any', 'humanity', 'they', 'once', 'had', 'is', 'now', 'gone', ',', 'but', 'by', 'god', ',', '\"', 'american', 'history', 'x', '\"', 'does', 'it', ',', 'and', 'for', 'that', ',', 'i', 'commend', 'it', '.', 'it', 'not', 'only', 'takes', 'balls', 'but', 'intelligence', 'to', 'make', 'a', 'human', 'being', 'out', 'of', 'a', 'neo', '-', 'nazi', 'skinhead', ',', 'a', 'kind', 'of', 'person', 'who', 'dedicates', 'their', 'lives', 'to', 'hating', 'anyone', 'who', \"'\", 's', 'not', 'what', 'they', 'are', ',', 'and', 'this', 'film', 'wisely', 'and', 'miracurously', 'pulls', 'it', 'off', '.', 'the', 'subject']\n",
            "Category: pos\n",
            "Accuracy: 84.25%\n",
            "Review: This movie was absolutely fantastic! The story was engaging and the characters were well-developed.\n",
            "Sentiment: pos\n",
            "Review: The movie was terrible. The plot was boring and the acting was awful.\n",
            "Sentiment: neg\n",
            "Review: An average movie with some good moments but overall not very memorable.\n",
            "Sentiment: pos\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "71IKxuFFInHT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6O-3jwb_InJ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rC6oWlr7InNA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9K2yM6boInP6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nMMlVvflInSj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CjZ1lWjmInVj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}